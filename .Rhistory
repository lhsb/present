qnorm(0.95)
qnorm(0.975)
m + c(-1,1) * qnorm(0.975) * s/sqrt(n)
c(-1,1) * qnorm(0.975) * s/sqrt(n)
s/sqrt(n)
m = 38
s = 14
n = 100
m + c(-1,1) * qnorm(0.975) * s/sqrt(n)
s/sqrt(n)
p = 0.37
n = 1000
sqrt(n)*p*(1-p)
sqrt(n*p*(1-p))
p*(1-p)/sqrt(n)
sqrt(p*(1-p)/n)
curve(sqrt(p*(1-p)/1000),from=0, to=1)
curve(sqrt(x*(1-x)/1000),from=0, to=1)
abline(v=0.5, col='red')
curve(sqrt(x*(1-x)/1000),from=0, to=1, xlab="p", ylab="SE")
abline(v=0.5, col='red')
(SE = sqrt(0.5 * (1 - 0.5) / n))
m + c(-1,1) * qnorm(0.975) * SE
setwd('~/s')
library(shiny)
install.packages("shiny")
library(shiny)
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
qnorm(0.975)
ci = 0.95
ci + (1 - ci) / 2
qnorm(0.975)
1.96^2*0.5^2/0.03^2
(qnorm(0.975)^2 * 0.5^2 / (3 / 100)^2) * 100
3 / 100
(3 / 100)^2
0.03^2
qnorm(0.975)^2 * 0.5^2
1.96^2*0.5^2
1.96^2*0.5^2/0.03^2
(qnorm(0.975)^2 * 0.5^2 / (3 / 100)^2)
(qnorm(0.975)^2 * 0.5^2 / (3 / 100)^2)
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/lenc/trading/R/shiny_app/idea_test1/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
runApp("/home/lukas/s/")
library(shinyapps)
Sys.setlocale(locale="en_US.UTF-8")
shinyapps::setAccountInfo(name="tsdata5", token="B558D12D9F908382A1858049F93918F0", secret="rAGNKAe8ZyLntff2WQeLkBliusxh9W4DNQTNNp5p")
getwd()
deployApp()
setwd(("/home/lukas/sample_size//"))
deployApp()
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
getwd()
deployApp()
deployApp()
runApp("/home/lukas/sample_size//")
deployApp()
deployApp()
system('grep "Margin of Error" -n *')
setwd('~//lic//R')
system('grep "Margin of Error" -n *')
system('grep "Margin" -n *')
system('grep "me" -n *')
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
runApp("/home/lukas/sample_size//")
setwd('~/sample_size/')
getwd()
setwd('~/sample_size//')
getwd()
deployApp()
grep(a, 'a')
grep('a',a)
grep('a',a[1])
a[1]
a = c("apple", 'bannana', 'cherry', 'lemon')
grep('a',a[1])
grep('a',a)
a[grep('a',a)]
2^7
library(slidify)
setwd('~/slid/')
author('pres')
library(twitteR)
install.packages(c("twitteR", "ROAuth"))
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(ggplot2)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL = "https://api.twitter.com/oauth/access_token"
authURL = "https://api.twitter.com/oauth/authorize"
consumerKey = "VtwM2JKNNOHbSAjRqviHVhA84"
consumerKey = "VtwM2JKNNOHbSAjRqviHVhA84"
consumerSecret = "ApBozmv1k1rAQYOzaysKkD8af67noPehzYWGXQjX7aj8X9qOCM"
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
Cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl") )
setwd('~/lic/R/')
save(Cred, file="twitter authentication.Rdata")
registerTwitterOAuth(Cred)
Rangers.list <- searchTwitter('#Coursera', n=1000, cainfo="cacert.pem")
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="cacert.pem")
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="~/lic//R//cacert.pem")
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="~/lic//R//cacert.pem")
load("~/lic/R/stwitter authentication.Rdata")
load("~/lic/R/twitter authentication.Rdata")
registerTwitterOAuth(Cred)
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="~/lic//R//cacert.pem")
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="~/lic/R/cacert.pem")
searchTwitter('#Coursera', n=1000)
Coursera.list <- searchTwitter('#Coursera', n=1000, cainfo="~/lic/R/cacert.pem")
Coursera.list <- searchTwitter('#Coursera', n=1000)
Coursera.df = twListToDF(Coursera.list)
write.csv(Coursera.df, file='~/slid//analysis//CourseraTweets.csv', row.names=F)
bigdata.list <- searchTwitter('#bigdata', n=1000)
bigdata.df = twListToDF(bigdata.list)
write.csv(bigdata.df, file='~/slid//analysis//bigdataTweets.csv', row.names=F)
rstats.list <- searchTwitter('#rstats', n=1000)
rstats.df = twListToDF(rstats.list)
write.csv(rstats.df, file='~/slid//analysis//rstatsTweets.csv', row.names=F)
Coursera.list <- searchTwitter('#shiny', n=1000)
bigdata.list <- searchTwitter('#Data', n=1000)
Coursera.list <- searchTwitter('#dataanalysis', n=1000)
android.list <- searchTwitter('#android', n=1000)
iphone.list <- searchTwitter('#iphone', n=1000)
excel.list <- searchTwitter('#excel', n=1000)
rstudio.list <- searchTwitter('#rstudio', n=1000)
rstats.list <- searchTwitter('#rstats', n=1000)
MOOC.list <- searchTwitter('#MOOC', n=1000)
rstats.df = twListToDF(rstats.list)
write.csv(rstats.df, file='~/slid//analysis//rstatsTweets.csv', row.names=F)
Coursera.list <- searchTwitter('#Coursera', n=1000)
Coursera.df = twListToDF(Coursera.list)
write.csv(Coursera.df, file='~/slid//analysis//CourseraTweets.csv', row.names=F)
bigdata.list <- searchTwitter('#bigdata', n=1000)
bigdata.df = twListToDF(bigdata.list)
write.csv(bigdata.df, file='~/slid//analysis//bigdataTweets.csv', row.names=F)
rstats.list <- searchTwitter('#rstats', n=1000)
rstats.df = twListToDF(rstats.list)
write.csv(rstats.df, file='~/slid//analysis//rstatsTweets.csv', row.names=F)
library (plyr)
library (stringr)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
hu.liu.pos = scan('~/slid//analysis/positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('~/slid//analysis/negative-words.txt', what='character', comment.char=';')
pos.words = c(hu.liu.pos, 'upgrade')
neg.words = c(hu.liu.neg, 'wtf', 'wait','waiting', 'epicfail', 'mechanical')
DatasetCoursera <- read.csv("~/slid//analysis/CourseraTweets.csv")
DatasetCoursera$text<-as.factor(DatasetCoursera$text)
DatasetCoursera
Datasetbigdata <- read.csv("~/slid//analysis/bigdataTweets.csv")
Datasetbigdata$text<-as.factor(Datasetbigdata$text)
Datasetrstats <- read.csv("~/slid//analysis/rstatsTweets.csv")
Datasetrstats$text<-as.factor(Datasetrstats$text)
Coursera.scores = score.sentiment(DatasetCoursera$text, pos.words,neg.words, .progress='text')
bigdata.scores = score.sentiment(Datasetbigdata$text, pos.words,neg.words, .progress='text')
rstats.scores = score.sentiment(Datasetrstats$text, pos.words,neg.words, .progress='text')
path<-"~/slid//analysis/"
write.csv(Coursera.scores,file=paste(path,"CourseraScores.csv",sep=""),row.names=TRUE)
write.csv(bigdata.scores,file=paste(path," bigdataScores.csv",sep=""),row.names=TRUE)
write.csv(rstats.scores,file=paste(path,"rstatsScores.csv",sep=""),row.names=TRUE)
Coursera.scores$Team = 'Coursera'
bigdata.scores$Team = 'bigdata'
rstats.scores$Team = 'rstats'
hist(Coursera.scores$score)
qplot(Coursera.scores$score)
hist(bigdata.scores$score)
qplot(bigdata.scores$score)
hist(rstats.scores$score)
qplot(rstats.scores$score)
#################################
#Chunk -6- Comparing 3 data sets
#################################
all.scores = rbind(Coursera.scores, bigdata.scores, rstats.scores)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Team), binwidth=1) +
facet_grid(Team~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
hist(rstats.scores$score)
qplot(rstats.scores$score)
all.scores = rbind(Coursera.scores, bigdata.scores, rstats.scores)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
####################################################
hu.liu.pos = scan('~/slid//analysis/positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('~/slid//analysis/negative-words.txt', what='character', comment.char=';')
#Add words to list
pos.words = c(hu.liu.pos, 'upgrade')
neg.words = c(hu.liu.neg, 'wtf', 'wait','waiting', 'epicfail', 'mechanical')
#Import 3 csv
DatasetCoursera <- read.csv("~/slid//analysis/CourseraTweets.csv")
DatasetCoursera$text<-as.factor(DatasetCoursera$text)
Datasetbigdata <- read.csv("~/slid//analysis/bigdataTweets.csv")
Datasetbigdata$text<-as.factor(Datasetbigdata$text)
Datasetrstats <- read.csv("~/slid//analysis/rstatsTweets.csv")
Datasetrstats$text<-as.factor(Datasetrstats$text)
#Score all tweets
Coursera.scores = score.sentiment(DatasetCoursera$text, pos.words,neg.words, .progress='text')
bigdata.scores = score.sentiment(Datasetbigdata$text, pos.words,neg.words, .progress='text')
rstats.scores = score.sentiment(Datasetrstats$text, pos.words,neg.words, .progress='text')
path<-"~/slid//analysis/"
write.csv(Coursera.scores,file=paste(path,"CourseraScores.csv",sep=""),row.names=TRUE)
write.csv(bigdata.scores,file=paste(path," bigdataScores.csv",sep=""),row.names=TRUE)
write.csv(rstats.scores,file=paste(path,"rstatsScores.csv",sep=""),row.names=TRUE)
Coursera.scores$Hashtag = 'Coursera'
bigdata.scores$Hashtag = 'bigdata'
rstats.scores$Hashtag = 'rstats'
hist(Coursera.scores$score)
qplot(Coursera.scores$score)
all.scores = rbind(Coursera.scores, bigdata.scores, rstats.scores)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
library(twitteR)
library(sentiment)
download.file("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz", "sentiment.tar.gz")
install.packages("sentiment.tar.gz", repos=NULL, type="source")
library(tm)
install.packages("tm")
download.file("http://cran.cnr.berkeley.edu/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz", "Rstem_0.4-1.tar.gz")
install.packages("Rstem_0.4-1.tar.gz", repos=NULL, type="source")
install.packages("sentiment.tar.gz", repos=NULL, type="source")
library(sentiment)
library(ggplot2)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
Coursera_txt = sapply(Coursera.list, function(x) x$getText())
Coursera_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", Coursera_txt)
Coursera_txt = gsub("@\\w+", "", Coursera_txt)
Coursera_txt = gsub("[[:punct:]]", "", Coursera_txt)
Coursera_txt = gsub("[[:digit:]]", "", Coursera_txt)
Coursera_txt = gsub("http\\w+", "", Coursera_txt)
Coursera_txt = gsub("[ \t]{2,}", "", Coursera_txt)
Coursera_txt = gsub("^\\s+|\\s+$", "", Coursera_txt)
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
Coursera_txt = sapply(Coursera_txt, try.error)
# remove NAs in Coursera_txt
Coursera_txt = Coursera_txt[!is.na(Coursera_txt)]
names(Coursera_txt) = NULL
#classify emotion
class_emo = classify_emotion(Coursera_txt, algorithm="bayes", prior=1.0)
#get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(Coursera_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
# data frame with results
sent_df = data.frame(text=Coursera_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
# sort data frame
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
# plot distribution of emotions
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of tweets",
title = "Sentiment Analysis of Tweets about Coursera\n(classification by emotion)",
plot.title = element_text(size=12))
# plot distribution of polarity
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets",
title = "Sentiment Analysis of Tweets about Coursera\n(classification by polarity)",
plot.title = element_text(size=12))
setwd('~/slid/')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
all.scores = rbind(Coursera.scores, bigdata.scores, rstats.scores)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of tweets",
title = "Sentiment Analysis of Tweets about Coursera\n(classification by emotion)",
plot.title = element_text(size=12))
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets",
title = "Sentiment Analysis of Tweets about Coursera\n(classification by polarity)",
plot.title = element_text(size=12))
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
write.csv(x=all.scores,'all_scores.csv')
View(all.scores)
View(all.scores)
save(all.scores,'all_scores')
ls()
ls('all_scores.csv')
save(list = all.scores, file = "all_scores.RData")
all.scores
ls()
ls(name='all.scores')
save(list = ls(name='all.scores'), file = "all_scores.RData")
save(list = list('all.scores'), file = "all_scores.RData")
ls()[7]
ls()[2]
save(list = ls()[2], file = "all_scores.RData")
save(list = ls()[2], file = "all_scores")
source('~/.active-rstudio-document')
source.with.encoding('~/slid/all_scores.csv', encoding='UTF-8')
source('~/.active-rstudio-document')
library(ggplot2)
save(list = ls()[2], file = "all_scores.RData")
all.scores = read.csv('https://dl.dropboxusercontent.com/u/198110768/all_scores.csv')
list.of.packages <- c("ggplot2", "RCurl")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(ggplot2)
library(RCurl)
url <- getURL("https://dl.dropboxusercontent.com/u/198110768/all_scores.csv")
all.scores <- read.csv(text = url)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
list.of.packages <- c("ggplot2", "RCurl")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(ggplot2)
library(RCurl)
url <- getURL("https://dl.dropboxusercontent.com/u/198110768/all_scores.csv")
all.scores <- read.csv(text = url)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~.) + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
slidify('pres/index.Rmd')
setwd('~/slid/')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
slidify('pres/index.Rmd')
png("world.png",height=500,width=850)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~., scales='free', space='free') + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
dev.off()
slidify('pres/index.Rmd')
all.scores = rbind(Coursera.scores, bigdata.scores, rstats.scores)
ggplot(data=all.scores) + # ggplot works on data.frames, always
geom_bar(mapping=aes(x=score, fill=Hashtag), binwidth=1) +
facet_grid(Hashtag~., space='free', scales='free') + # make a separate plot for each hashtag
theme_bw() + scale_fill_brewer() # plain display, nicer colors
publish_github('lhsb', 'present')
slidify('pres/index.Rmd')
publish_github('lhsb', 'present')
publish_github(user='lhsb', repo='present')
mode        : selfcontained # {standalone, draft}
publish(user = "lhsb", repo = "present", host = 'github')
setwd('~/slid/pres')
publish_github(user='lhsb', repo='present')
publish_github(user='lhsb', repo='https://github.com/lhsb/present/')
publish_github(user='lhsb', repo='present')
publish(user = "lhsb", repo = "present", host = 'github')
publish_github(user='lhsb', repo='present.git')
slidify('pres/index.Rmd')
setwd('~/slid/pres')
slidify('pres/index.Rmd')
setwd('~/slid/pres')
slidify('~/pres/index.Rmd')
slidify('~/slid/pres/index.Rmd')
publish_github(user='lhsb', repo='present')
publish_github(user='lhsb', repo='present')
install_github("slidify", "ramnathv")
require(devtools)
install_github("slidify", "ramnathv")
library(slidify)
slidify('~/slid/pres/index.Rmd')
publish_github(user='lhsb', repo='present')
